
"""
Training Protocol for CODN-350M.
"""
import os
import sys
import logging
import torch
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import GradScaler
# Note: In a real environment, tokenizers import needs to be verified
from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders
from CODN_350M.model import CODN350M
from CODN_350M.model_config import CODNConfig

# Add backend to path for imports
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))


# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ProtocolEnforcer:
    """
    Enforces the 'Zero-Error' protocol constraints (Hardware, KPI, Data).
    """

    def __init__(self):
        self.max_vram_gb = 6.0  # RTX 4050 Constraint
        self.target_precision = 0.9995

    def check_hardware(self):
        """Checks if hardware meets minimal requirements."""
        if not torch.cuda.is_available():
            logger.warning(
                "CUDA not available! Training will be extremely slow.")
            print("WARNING: CUDA not available!")
            return False

        vram = torch.cuda.get_device_properties(0).total_memory / 1e9
        logger.info("Detected VRAM: %.2f GB", vram)
        print(f"Detected VRAM: {vram:.2f} GB")
        if vram < 4.0:
            logger.error("Insufficient VRAM for this protocol.")
            print("ERROR: Insufficient VRAM.")
            return False
        return True


class TextDataset(Dataset):
    """
    Dataset for Causal Language Modeling.
    """

    def __init__(self, file_path, tokenizer, block_size=512):
        self.examples = []
        if not os.path.exists(file_path):
            logger.error("Data file not found: %s", file_path)
            return

        with open(file_path, "r", encoding="utf-8") as f:
            lines = f.readlines()

        for line in lines:
            line = line.strip()
            if not line:
                continue
            encoded = tokenizer.encode(line)
            # Truncate
            ids = encoded.ids
            if len(ids) > block_size:
                ids = ids[:block_size]
            else:
                # Pad
                ids = ids + [0] * (block_size - len(ids))
            self.examples.append(torch.tensor(ids, dtype=torch.long))

    def __len__(self):
        return len(self.examples)

    def __getitem__(self, i):
        return self.examples[i]


def train_tokenizer(files, vocab_size=32000):
    """Trains a BPE tokenizer on the given files."""
    logger.info("Training Tokenizer...")
    print("Training Tokenizer...")
    tokenizer = Tokenizer(models.BPE())
    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)
    tokenizer.decoder = decoders.ByteLevel()

    trainer = trainers.BpeTrainer(
        vocab_size=vocab_size,
        special_tokens=["[PAD]", "[UNK]", "[CLS]", "[SEP]",
                        "[MASK]", "[LOGIC_GATE]", "TRUE", "FALSE",
                        "[RISK]", "[BOTTLENECK]", "[CLEAN]",
                        "[ACTION_REQUIRED]"],
        initial_alphabet=pre_tokenizers.ByteLevel.alphabet()
    )

    tokenizer.train(files, trainer)
    return tokenizer


def train_codn_protocol():
    """Main training protocol."""
    print("Starting CODN-350M Training Protocol...")
    enforcer = ProtocolEnforcer()
    if not enforcer.check_hardware():
        print("Hardware check failed.")
        return

    # 1. Data Foundry
    logger.info("PHASE 1: Data Foundry Initialization")
    print("PHASE 1: Data Foundry Initialization")

    # Use the Golden Truth Dataset generated by dataset_gen.py
    data_path = "synthetic_corpus_v1.txt"

    if not os.path.exists(data_path):
        logger.error(
            "Golden Truth Dataset not found at %s. Please run dataset_gen.py first.", data_path)
        print("ERROR: synthetic_corpus_v1.txt not found. Run dataset_gen.py")
        return

    # 2. Tokenizer
    logger.info("PHASE 2: Tokenization")
    print("PHASE 2: Tokenization")
    tokenizer = train_tokenizer([data_path])
    vocab_size = tokenizer.get_vocab_size()
    print(f"Tokenizer trained. Vocab size: {vocab_size}")

    # 3. Model
    logger.info("PHASE 3: Model Initialization (350M Configuration)")
    print("PHASE 3: Model Initialization (350M Configuration)")

    # Load Strict Configuration
    config = CODNConfig(vocab_size=vocab_size)

    device = torch.device(config.device)

    model = CODN350M(
        vocab_size=config.vocab_size,
        d_model=config.d_model,
        n_layers=config.n_layers,
        n_heads=config.n_heads,
        max_seq_len=config.max_seq_len,
        dropout=config.dropout
    ).to(device)

    # 4. Training (Mixed Precision + Gradient Accumulation)
    print("PHASE 4: Training Loop Start")
    # Identify Critical Tokens for Weighted Loss
    critical_tokens = ["[RISK]", "[BOTTLENECK]",
                       "[CLEAN]", "[ACTION_REQUIRED]"]
    critical_ids = [tokenizer.token_to_id(t) for t in critical_tokens
                    if tokenizer.token_to_id(t) is not None]

    loss_weights = torch.ones(vocab_size, device=device)
    for cid in critical_ids:
        loss_weights[cid] = 100.0
        print(f"Assigning 100x weight to token ID: {cid}")

    criterion = torch.nn.CrossEntropyLoss(weight=loss_weights)

    dataset = TextDataset(data_path, tokenizer, block_size=config.max_seq_len)
    print(f"Dataset loaded. {len(dataset)} examples.")
    # Optimized batch size for 256 seq len - Conservative for 6GB VRAM
    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)

    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)
    scaler = GradScaler()

    model.train()
    accumulation_steps = 8       # Balance for effective batch size 64

    print("Starting Epochs...")
    for epoch in range(3):
        for step, batch in enumerate(dataloader):
            inputs = batch[:, :-1].to(device)
            targets = batch[:, 1:].to(device)

            # Enforce bfloat16 for RTX 40-series optimization
            with torch.autocast(device_type='cuda', dtype=torch.bfloat16):
                logits, _ = model(inputs, targets=None)
                # Reshape for CrossEntropy: (B*T, V) vs (B*T)
                loss = criterion(
                    logits.view(-1, vocab_size),
                    targets.view(-1)
                )
                loss = loss / accumulation_steps

            scaler.scale(loss).backward()

            if (step + 1) % accumulation_steps == 0:
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()

            if step % 10 == 0:
                log_msg = (f"Epoch {epoch} Step {step} Loss: "
                           f"{loss.item() * accumulation_steps:.4f}")
                logger.info(log_msg)
                print(log_msg)

    torch.save(model.state_dict(), "CODN_350M_Phase1.pt")
    logger.info("Model saved to CODN_350M_Phase1.pt")
    print("Model saved to CODN_350M_Phase1.pt")


if __name__ == "__main__":
    train_codn_protocol()
